# アーキテクチャ決定: シングルスレッド・Shared-Nothing設計

## 作成日
2025-10-17

## 決定事項

BenchFSは**シングルスレッド・シングルプロセス・Shared-Nothing**アーキテクチャで実装します。

---

## アーキテクチャ概要

### 基本原則

1. **サーバー**: 各サーバーはシングルスレッド・シングルプロセス
2. **クライアント**: 各クライアントもシングルスレッド・シングルプロセス
3. **スケーリング**: 水平スケール (プロセス数を増やす)
4. **通信**: 全てUCX経由 (ノード内/ノード間を区別しない)
5. **Shared-Nothing**: プロセス間でメモリ共有なし

### 通信モデル

```
┌─────────────────────────────────────────────────────────┐
│                       Node 1                            │
│  ┌──────────┐   ┌──────────┐   ┌──────────┐          │
│  │ Server 1 │   │ Server 2 │   │ Client 1 │          │
│  │(Process) │   │(Process) │   │(Process) │          │
│  └────┬─────┘   └────┬─────┘   └────┬─────┘          │
│       │              │              │                  │
│       └──────UCX─────┴──────UCX─────┘                 │
│                 (Shared Memory)                        │
└─────────────────────────────────────────────────────────┘
                       │
                      UCX
                (InfiniBand/RoCE)
                       │
┌─────────────────────────────────────────────────────────┐
│                       Node 2                            │
│  ┌──────────┐   ┌──────────┐                          │
│  │ Server 3 │   │ Server 4 │                          │
│  │(Process) │   │(Process) │                          │
│  └────┬─────┘   └────┬─────┘                          │
│       └──────UCX──────┘                                │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

### UCXの利点

UCXは通信相手の位置に応じて自動的に最適なトランスポートを選択:

- **同一ノード内**: Shared Memory (CMA, KNEM, XPMEM)
- **ノード間**: InfiniBand/RoCE (RDMA)
- **フォールバック**: TCP/IP

この透過性により、アプリケーションコードは通信相手の位置を意識する必要がない。

---

## 設計上の利点

### 1. シンプルさ

**利点**:
- ロック不要 (データ競合なし)
- アトミック操作不要
- デバッグが容易
- 実装が直感的

**理由**:
- 各プロセスは独立したアドレス空間を持つ
- プロセス内は完全にシングルスレッド
- 非同期I/Oはイベントループで処理 (Pluvio runtime)

### 2. スケーラビリティ

**水平スケール**:
- ノードあたり複数のサーバープロセスを起動
- CPU コアごとに1プロセスを配置可能
- NUMA-aware配置が容易

**例** (48コアノード):
```
Node 1:
  - Server Process 1-48 (各コアに1プロセス)
  - 各プロセスは独立したストレージパスを管理
  - UCXで相互通信
```

### 3. UCXとの相性

**Shared Memory最適化**:
- 同一ノード内のプロセス間通信はゼロコピー
- カーネルバッファ経由でデータ転送
- RDMA NIC不要でも高速

**RDMA最適化**:
- ノード間はRDMAで直接メモリアクセス
- CPU介在を最小化
- レイテンシとスループットが優れる

### 4. 障害分離

**プロセス独立性**:
- 1プロセスのクラッシュが他プロセスに影響しない
- プロセス単位での再起動が容易
- デバッグ時に1プロセスのみ停止可能

### 5. NUMA-Aware配置

**メモリローカリティ**:
```rust
// 各サーバープロセスは特定のNUMAノードに配置
Server Process 1 → NUMA Node 0 → CPU 0-23  → Local Memory
Server Process 2 → NUMA Node 1 → CPU 24-47 → Local Memory
```

- ローカルメモリアクセスで性能向上
- リモートメモリアクセス (QPI/UPI) を回避

---

## マルチスレッドとの比較

### マルチスレッドの課題

| 項目 | マルチスレッド | シングルスレッド・マルチプロセス |
|------|---------------|--------------------------------|
| ロック | 必要 (Mutex/RwLock) | 不要 |
| アトミック操作 | 頻繁に必要 | 不要 |
| キャッシュコヒーレンシ | オーバーヘッド大 | プロセス独立 |
| デバッグ | 困難 (競合状態) | 容易 |
| スケーラビリティ | コア数に限界 | ノード数まで拡張 |
| 障害分離 | スレッドクラッシュ→プロセス全体 | プロセス単位で独立 |

### マルチスレッドが有利な場合

以下のケースではマルチスレッドが有利:

1. **メモリ共有が本質的に必要**
   - 例: 巨大な共有キャッシュ (数GB以上)
   - 理由: プロセス間共有メモリはオーバーヘッド大

2. **レイテンシが極めて重要 (マイクロ秒単位)**
   - 例: リアルタイムトレーディングシステム
   - 理由: プロセス間通信は最低でも数マイクロ秒

3. **頻繁な小さいデータ交換**
   - 例: スレッド間でカウンタを共有
   - 理由: アトミック変数の方が高速

### BenchFSにおける判断

BenchFSの特性:
- **大きいデータブロック** (4MBチャンク)
- **ファイルシステム操作** (ms単位のレイテンシ)
- **I/O律速** (CPU律速ではない)

→ **シングルスレッド・マルチプロセスが最適**

---

## 実装上の変更

### 1. `Send + Sync` 要件の削除

**変更前**:
```rust
pub trait StorageBackend: Send + Sync {
    // ...
}
```

**変更後**:
```rust
pub trait StorageBackend {
    // ...
}
```

**理由**:
- シングルスレッドなのでスレッド間送信は不要
- `Rc<T>` の使用が可能 (pluvio_uringとの互換性)

### 2. `Rc` の使用

**方針**:
- プロセス内の共有には `Rc<T>` を使用
- `Arc<T>` は不要 (オーバーヘッドを避ける)

**例**:
```rust
pub struct LocalFileSystem {
    backend: Rc<IOUringBackend>,  // Arc -> Rc
    // ...
}
```

### 3. 非同期ランタイム

**Pluvio Runtime**:
- シングルスレッドイベントループ
- IOURING/UCXをリアクターとして登録
- 協調的マルチタスキング

---

## 性能見積もり

### プロセス間通信 (UCX Shared Memory)

- **レイテンシ**: 1-2 μs
- **スループット**: 10-20 GB/s (同一ノード内)

### RDMA (InfiniBand/RoCE)

- **レイテンシ**: 1-5 μs
- **スループット**: 100-200 Gbps (ノード間)

### ファイルI/O (IOURING)

- **NVMe SSD**: 500K IOPS, 3-7 GB/s
- **レイテンシ**: 100-200 μs

### 結論

**ボトルネック**: ファイルI/O (100μs)

**プロセス間通信オーバーヘッド**: 1-2μs (1-2%)

→ **シングルスレッド設計でも性能劣化はほぼゼロ**

---

## 将来的な検討事項

### マルチスレッドを検討すべきケース

以下の状況が発生した場合、マルチスレッドを再検討:

1. **巨大なメタデータキャッシュの共有が必要**
   - サイズ: 10GB以上
   - 頻繁なアクセス (ms単位)
   - → 検証: キャッシュミス率 vs プロセス間通信コスト

2. **CPUバウンドな処理が支配的**
   - 例: 暗号化/復号化、圧縮/展開
   - → 検証: CPU使用率 > 80%

3. **極めて高いリクエストレート**
   - 例: 1M+ requests/sec per process
   - → 検証: イベントループがボトルネック

### 提案のフォーマット

マルチスレッド化を提案する際は、以下の情報を含める:

```markdown
## マルチスレッド化提案

### 背景
- 現状の問題: [具体的な性能ボトルネック]
- 測定データ: [ベンチマーク結果]

### 提案内容
- マルチスレッド化する範囲: [モジュール名]
- 並行度: [スレッド数]
- 同期機構: [Mutex/RwLock/Atomic]

### 性能見積もり
- 期待される改善: [X% 向上]
- オーバーヘッド: [ロックコンテンション、キャッシュミス]

### トレードオフ
- 複雑性の増加: [デバッグ難易度、バグリスク]
- 代替案: [プロセス数を増やす、アルゴリズム改善]
```

---

## まとめ

### 採用する設計

- **シングルスレッド・シングルプロセス**
- **水平スケール** (プロセス数増加)
- **UCX統一通信** (Shared Memory + RDMA)
- **Shared-Nothing** (プロセス独立)

### 理由

1. **シンプルさ**: ロック不要、デバッグ容易
2. **スケーラビリティ**: ノード数まで拡張可能
3. **UCXとの相性**: Shared Memory最適化
4. **NUMA-Aware**: ローカルメモリアクセス
5. **障害分離**: プロセス単位で独立

### 性能

- **ボトルネック**: ファイルI/O (100μs)
- **通信オーバーヘッド**: 1-2μs (無視できる)
- **期待性能**: マルチスレッド版と同等以上

---

## 参考資料

- [UCX Documentation](https://openucx.readthedocs.io/)
- [Pluvio Runtime](https://github.com/maetin0324/pluvio)
- [The C10M Problem](http://c10m.robertgraham.com/p/manifesto.html)
- [Shared-Nothing Architecture](https://en.wikipedia.org/wiki/Shared-nothing_architecture)
