# BenchFS vs CHFS I/Oパターン比較分析

## 概要

BenchFSのREAD性能がWRITE性能の1/7程度に低下する問題を調査する中で、CHFSとのI/Oパターンの比較分析を行った。

---

## 1. アーキテクチャ比較

### チャンキング方式

| 項目 | BenchFS | CHFS |
|------|---------|------|
| チャンクサイズ | **4 MiB** (固定) | 64 KiB (デフォルト、可変) |
| チャンク命名 | `<base_dir>/<path_hash>/<shard>/<chunk_index>` | `<path>:<chunk_index>` (例: `/data/file:1`) |
| ディレクトリ構造 | **256-way シャーディング** | フラット（論理パスをそのまま使用） |
| ファイル数 | 大量（シャードごとに分散） | 少ない（チャンクインデックスのみ追加） |

### I/O方式

| 項目 | BenchFS | CHFS |
|------|---------|------|
| I/Oライブラリ | **io_uring** + registered buffers | pread/pwrite (+ オプションでABT-IO) |
| O_DIRECT | **常に有効** | **無効**（デフォルト） |
| ゼロコピー | 登録済みバッファでDMA転送 | なし |
| 非同期I/O | io_uring完全非同期 | 同期 or ABT-IOスレッドプール |

### ファイルハンドル管理

| 項目 | BenchFS | CHFS |
|------|---------|------|
| キャッシュ | **LRUキャッシュ** (262,144ハンドル) | なし（毎回open/close） |
| オープン戦略 | O_RDWR で開き、キャッシュに保持 | 必要時にopen、即座にclose |
| エビクション | LRU方式、遅延クローズ | N/A |

---

## 2. ファイルレイアウトの違い

### BenchFSのディスク構造

```
/scr/data/
  └── a1b2c3d4e5f6g7h8/          # path_hash
      ├── 00/                     # shard (chunk_index % 256)
      │   ├── 0                   # chunk_index=0
      │   ├── 256                 # chunk_index=256
      │   └── 512                 # chunk_index=512
      ├── 01/
      │   ├── 1
      │   ├── 257
      │   └── ...
      └── ff/
          └── 255
```

**16 GiB × 736クライアントの場合:**
- チャンク数: 16 GiB / 4 MiB = 4,096 チャンク/ファイル
- 総チャンク数: 736 × 4,096 = **3,014,656 チャンク**
- ディレクトリ数: 736 パスハッシュ × 256 シャード = **188,416 ディレクトリ**

### CHFSのディスク構造

```
/cache/benchmark/
  └── data.bin                    # chunk 0 (メタデータ + データ)
  └── data.bin:1                  # chunk 1
  └── data.bin:2                  # chunk 2
  └── ...
```

**同じワークロードの場合:**
- チャンク数: 同程度（チャンクサイズが異なる場合は異なる）
- ディレクトリ数: **元のパス構造のみ**（数百〜数千）

---

## 3. I/Oパターン分析

### WRITE時の動作

**BenchFS:**
1. クライアントからRPCでデータ受信
2. 登録済みバッファにデータをコピー
3. チャンクファイルパスを計算（hash + shard）
4. ディレクトリがなければ作成
5. `DmaFile::open_with_reactor()` でファイルオープン（O_DIRECT | O_RDWR | O_CREAT）
6. `write_fixed()` でio_uring経由でpwrite
7. ファイルハンドルをLRUキャッシュに保持

**CHFS:**
1. `pwrite()` でデータ書き込み
2. OSページキャッシュに書き込み → 後でフラッシュ
3. ファイルをclose

### READ時の動作

**BenchFS:**
1. クライアントからRPCで読み取りリクエスト
2. チャンクファイルパスを計算
3. LRUキャッシュでファイルハンドルを検索
   - **キャッシュヒット**: 既存ハンドルを使用
   - **キャッシュミス**: 新規オープン（**O_DIRECT**）
4. `read_fixed()` でio_uring経由でpread
5. **O_DIRECTのため、毎回ディスクから読み取り**

**CHFS:**
1. `open()` でファイルオープン
2. `pread()` でデータ読み取り
   - **OSページキャッシュから読み取り可能**
3. `close()` でファイルクローズ

---

## 4. 性能差異の原因分析

### なぜBenchFSのWRITEは高速か？

1. **NVMe書き込みバッファ**: O_DIRECTでも、NVMe SSDのDRAMバッファに書き込まれるため高速
2. **io_uringのバッチ処理**: 複数のSQEをまとめて投入可能
3. **ファイルハンドルキャッシュ**: 同じチャンクへの再書き込みはキャッシュヒット

### なぜBenchFSのREADは低速か？

1. **O_DIRECTの影響**:
   - 毎回NANDフラッシュから読み取り
   - OSページキャッシュが使えない
   - 同じチャンクの再読み取りでもディスクアクセスが必要

2. **大量のファイル/ディレクトリ**:
   - 188,000+のディレクトリを作成
   - ディレクトリルックアップのオーバーヘッド
   - XFSのiノードキャッシュへの負荷

3. **LRUキャッシュミス問題**:
   - IOR READフェーズでは、WRITEで作成した全チャンクに順番にアクセス
   - 262,144ハンドル < 3,014,656チャンク → **100% キャッシュミス**
   - 毎回ファイルオープンが必要

4. **ランダムアクセスパターン**:
   - 736クライアントが異なるチャンクに同時アクセス
   - ディスクシーク（論理的）が発生
   - NVMeでも並列読み取りには限界あり

### iostatで観測された現象の説明

**観測結果:**
- WRITE: 6,500 IOPS/NVMe, w_await = 5-25 ms
- READ: 620 IOPS/NVMe, r_await = 0.7-427 ms（ノードにより異なる）

**説明:**
1. **IOPS差（10倍）**: WRITEはバッファリング可能、READは毎回ディスク読み取り
2. **r_await悪化**: ディレクトリルックアップとiノードアクセスのオーバーヘッド
3. **ノード間差異**: XFSのiノードキャッシュ状態、mdadm RAIDの状態による

---

## 5. CHFSが高速な理由（推測）

1. **OSページキャッシュ活用**: O_DIRECTを使わないため、READもキャッシュから返せる
2. **シンプルなファイル構造**: シャーディングなし、ディレクトリルックアップが軽量
3. **ファイル毎回close**: ファイルハンドル管理の複雑さがない

---

## 6. 改善案

### 短期的改善

1. **O_DIRECTの選択的無効化**:
   - READのみページキャッシュを使用
   - WRITEはO_DIRECTを維持（整合性のため）

2. **シャーディング戦略の見直し**:
   - 256シャードは過剰？
   - 16〜64シャードに削減してディレクトリ数を減らす

3. **LRUキャッシュサイズの最適化**:
   - ワークロードに応じて動的に調整
   - または、キャッシュを無効化して毎回open/closeに変更

### 中期的改善

1. **単一ファイルへのチャンク統合**:
   - CHFSのようにチャンクを単一ファイル内に配置
   - pread/pwriteでオフセットを計算
   - ディレクトリオーバーヘッドを排除

2. **ABT-IOの検討**:
   - CHFSと同様にArgobots非同期I/Oを使用
   - io_uringとの性能比較

### 長期的改善

1. **ハイブリッドキャッシュ**:
   - メタデータ（パスハッシュ、シャード計算）のキャッシュ
   - ホットチャンクのメモリキャッシュ

2. **適応的O_DIRECT**:
   - アクセスパターンに基づいてO_DIRECTを動的に切り替え

---

## 7. 結論

BenchFSのREAD性能問題は、**O_DIRECT + 大量ファイル/ディレクトリ + LRUキャッシュミス**の組み合わせによるものと考えられる。

CHFSは以下の点でシンプルなアプローチを採用している：
- OSページキャッシュの活用
- フラットなファイル構造
- 毎回のopen/close（キャッシュ管理の複雑さ回避）

BenchFSの高性能設計（io_uring、O_DIRECT、ゼロコピー）は、**シーケンシャルな大規模I/O**では有効だが、**多数の小ファイルへのランダムアクセス**では逆効果となる可能性がある。

---

## 参考資料

- BenchFS: `src/storage/chunk_store.rs`, `src/storage/iouring.rs`
- CHFS: `external/chfs/chfsd/fs_posix.c`, `external/chfs/chfsd/backend_local.c`
- [io_uring and O_DIRECT performance characteristics](https://kernel.dk/io_uring.pdf)
